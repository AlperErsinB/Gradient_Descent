import numpy as np
import matplotlib.pyplot as plt


X = 2 * np.random.rand(100, 1)
X

error = np.random.randn(100, 1)
error

y = 4 + 3 * X + error
y


# veriyi Ã§izelim

plt.plot(X,y,'b.')
plt.xlabel("$x$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
_ =plt.axis([0,2,0,15])


# X'e bias column ekle
X_b = np.c_[np.ones((100,1)),X]


# katsayÄ± tahminlerini hesapla -> numpy ile Ã§Ã¶z

w_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
print(w_best)



# yeni data -> X_new

X_new = np.array([[0],[2]])
X_new



# bias column ekle

X_new_b = np.c_[np.ones((2,1)),X_new]
X_new_b



# tahmin yap
# elimizde optimum w deÄŸerleri var -> w_best

y_predict = X_new_b.dot(w_best)
y_predict



# tahmin doÄŸrusu
plt.plot(X_new, y_predict, 'r-')

# bÃ¼tÃ¼n data
plt.plot(X, y, 'b.')

plt.xlabel("$x_1$", fontsize=18)
plt.ylabel("$y$", rotation=0, fontsize=18)
plt.axis([0,2,0,15])
plt.show()



# Maliyet (Cost) Hesaplama Fonksiyonu

def cal_cost(W, X, y):
    """
    X ve y iÃ§in Cost (Maliyet) hesaplar.
    Parametreler:
    W = KatsayÄ± vektÃ¶rÃ¼ -> (p, 1)
    X = Girdi vektÃ¶rÃ¼   -> (n, p)
    y = GerÃ§ek y'ler    -> (n, 1)
    Burada:
    n: Toplam veri adedi (satÄ±r sayÄ±sÄ±)
    p: X iÃ§indeki deÄŸiÅŸken adedi (sÃ¼tun sayÄ±sÄ±)
    
    DÃ¶nÃ¼ÅŸ:
    cost = hesaplanan maliyet deÄŸeri
    """
    
    n = len(y)
    
    # Ã¶nce tahmini hesapla -> ğ‘¦Ì‚ = X W
    # X -> (n, p)
    # W -> (p, 1)
    # ğ‘¦Ì‚ -> (n, 1)
    tahmin = X.dot(W)
    
    # cost function deÄŸerini hesapla -> J
    cost = (1/2*n) * np.sum(np.square(tahmin-y))
    
    # maliyet (cost) deÄŸerini dÃ¶n
    return cost
    
    
 # Gradient Descent Fonksiyonu

def gradient_descent(X, y, W, learning_rate=0.01, iterations=100):
    """
    Gradient Descent uygulama fonksiyonu.
    Parametreler:
    X = X Matrisi (bias unit eklenmiÅŸ hali, yani 1'lerden oluÅŸmuÅŸ ilk sÃ¼tun)
    y = y vektÃ¶rÃ¼
    W = KatsayÄ± vektÃ¶rÃ¼ (w'lardan oluÅŸmuÅŸ)
    learning_rate = learning rate: alpha (Ã¶ÄŸrenme katsayÄ±sÄ±)
    iterations = toplam dÃ¶ngÃ¼ sayÄ±sÄ±
    DÃ¶nÃ¼ÅŸ:
    * W vektÃ¶rÃ¼nÃ¼n son hali
    * Maliyet Listesi (cost history)
    * W vektÃ¶rÃ¼nÃ¼n listesi (weight history)
    """
    
    n = len(y)
    cost_history = np.zeros(iterations)
    w_history = np.zeros((iterations, 2))
    
    for it in range(iterations):
        
        tahmin = np.dot(X, W)
        
        W = W - (1/n) * learning_rate * (X.T.dot((tahmin - y)))
        
        w_history[it,:] = W.T
        
        cost_history[it] = cal_cost(W, X, y)
    
    return W, cost_history, w_history
 
 
 
 # learning rate
lr = 0.01

# iterasyon sayÄ±sÄ±
n_iter = 1000

# baÅŸlangÄ±Ã§ iÃ§in W vektÃ¶rÃ¼
W = np.random.randn(2,1)

# X vektÃ¶rÃ¼ne bias column ekle
X_b = np.c_[np.ones((len(X),1)),X]

# gradient descent Ã§alÄ±ÅŸtÄ±r
W, cost_history, w_history = gradient_descent(X_b, y, W, lr, n_iter)


print('W0: {:0.3f}'.format(W[0][0]))
print('W1: {:0.3f}'.format(W[1][0]))
print('Final cost/MSE: {:0.3f}'.format(cost_history[-1]))



fig,ax = plt.subplots(figsize=(12,8))

ax.set_ylabel('J(W)')
ax.set_xlabel('Iterations')

_=ax.plot(range(n_iter),cost_history,'b.')



fig,ax = plt.subplots(figsize=(10,8))

_=ax.plot(range(200),cost_history[:200],'b.')



# Gradient Descent'i Ã§izen fonksiyon

def plot_GD(n_iter, lr, ax, ax1=None):
     """
     n_iter = iterasyon sayÄ±sÄ±
     lr = Learning Rate
     ax = Gradient Descent Ã§izmek iÃ§in axis
     ax1 = cost_history vs Iterations grafiÄŸi iÃ§in axis
     """
     _ = ax.plot(X, y, 'b.')
     W = np.random.randn(2,1)

     tr = 0.1
     cost_history = np.zeros(n_iter)
        
     for i in range(n_iter):
        pred_prev = X_b.dot(W)
        
        W, h, _ = gradient_descent(X_b, y, W, lr, 1)
        
        pred = X_b.dot(W)

        cost_history[i] = h[0]

        if ((i % 25 == 0) ):
            _ = ax.plot(X, pred,'r-', alpha=tr)
            if tr < 0.8:
                tr = tr+0.2
                
     if not ax1== None:
        _ = ax1.plot(range(n_iter),cost_history,'b.')  
        
        
        
 
 fig = plt.figure(figsize=(30,25))
fig.subplots_adjust(hspace=0.4, wspace=0.4)

# iterasyon ve learning rate listesi
it_lr =[(2000,0.001),(500,0.01),(200,0.05),(100,0.1)]

count =0

for n_iter, lr in it_lr:
    count += 1
    
    ax = fig.add_subplot(4, 2, count)
    count += 1
   
    ax1 = fig.add_subplot(4, 2, count)
    
    ax.set_title("lr:{}".format(lr))
    ax1.set_title("Iterations:{}".format(n_iter))
    
    plot_GD(n_iter, lr, ax, ax1)
    
    
 
 _,ax = plt.subplots(figsize=(14,10))

plot_GD(100, 0.1, ax)




_,ax = plt.subplots(figsize=(14,10))

plot_GD(100, 0.01, ax)



# Stochastic Gradient Descent Fonksiyonu

def stochastic_gradient_descent(X, y, W, learning_rate=0.01, iterations=100):
    """
    Stochastic Gradient Descent uygulama fonksiyonu.
    Parametreler:
    X = X Matrisi (bias unit eklenmiÅŸ hali, yani 1'lerden oluÅŸmuÅŸ ilk sÃ¼tun)
    y = y vektÃ¶rÃ¼
    W = KatsayÄ± vektÃ¶rÃ¼ (w'lardan oluÅŸmuÅŸ)
    learning_rate = learning rate: alpha (Ã¶ÄŸrenme katsayÄ±sÄ±)
    iterations = toplam dÃ¶ngÃ¼ sayÄ±sÄ±
    DÃ¶nÃ¼ÅŸ:
    * W vektÃ¶rÃ¼nÃ¼n son hali
    * Maliyet Listesi (cost history)
    * W vektÃ¶rÃ¼nÃ¼n listesi (weight history)
    """
    
    n = len(y)    
    cost_history = np.zeros(iterations)
    
    for it in range(iterations):        
        cost = 0        
        # her seferinde rasgele bir X_i deÄŸeri seÃ§ip maliyet hesaplayacaÄŸÄ±z
        for i in range(n):            
            # rasgele bir deÄŸer al
            rand_ind = np.random.randint(0, n)    
            
            X_i = X[rand_ind,:].reshape(1, X.shape[1])            
            y_i = y[rand_ind].reshape(1,1)
        
            # tek X_i iÃ§in tahmin
            tahmin = np.dot(X_i, W)

            # tek X_i iÃ§in katsayÄ± deÄŸiÅŸimi
            W = W - (1/n) * learning_rate * (X_i.T.dot((tahmin - y_i)))
            
            # tek X_i iÃ§in hesaplanan cost'u cost deÄŸiÅŸkenine ekle
            cost += cal_cost(W, X_i, y_i)
        
        # bu iterasyon iÃ§in cost deÄŸerini cost_history'ye ekle
        cost_history[it] = cost
    
    return W, cost_history
   

# learning rate
lr = 0.5

# iterasyon sayÄ±sÄ±
n_iter = 50

# baÅŸlangÄ±Ã§ iÃ§in W vektÃ¶rÃ¼
W = np.random.randn(2,1)

# X vektÃ¶rÃ¼ne bias column ekle
X_b = np.c_[np.ones((len(X),1)),X]

# gradient descent Ã§alÄ±ÅŸtÄ±r
W, cost_history = stochastic_gradient_descent(X_b, y, W, lr, n_iter)


print('W0: {:0.3f}'.format(W[0][0]))
print('W1: {:0.3f}'.format(W[1][0]))
print('Final cost/MSE: {:0.3f}'.format(cost_history[-1]))



fig,ax = plt.subplots(figsize=(10,8))

ax.set_ylabel('{J(W)}',rotation=0)
ax.set_xlabel('{Iterations}')

_=ax.plot(range(n_iter), cost_history, 'b.')




# Mini-Batch Stochastic Gradient Descent Fonksiyonu

def minibatch_stochastic_gradient_descent(X, y, W, learning_rate=0.01, iterations=100, batch_size=20):
    """
    Stochastic Gradient Descent uygulama fonksiyonu.
    Parametreler:
    X = X Matrisi (bias unit eklenmiÅŸ hali, yani 1'lerden oluÅŸmuÅŸ ilk sÃ¼tun)
    y = y vektÃ¶rÃ¼
    W = KatsayÄ± vektÃ¶rÃ¼ (w'lardan oluÅŸmuÅŸ)
    learning_rate = learning rate: alpha (Ã¶ÄŸrenme katsayÄ±sÄ±)
    iterations = toplam dÃ¶ngÃ¼ sayÄ±sÄ±
    DÃ¶nÃ¼ÅŸ:
    * W vektÃ¶rÃ¼nÃ¼n son hali
    * Maliyet Listesi (cost history)
    * W vektÃ¶rÃ¼nÃ¼n listesi (weight history)
    """
    
    n = len(y)
    cost_history = np.zeros(iterations)
    # toplam kaÃ§ adet batch olacak
    n_batches = int(n / batch_size)
    
    for it in range(iterations):
        cost = 0
        # X ve y'leri karÄ±ÅŸtÄ±r (veri sÄ±ralarÄ± rasgele olsun)
        indices = np.random.permutation(n)
        X = X[indices]
        y = y[indices]
        
        # her seferinde rasgele bir X_i partisi alÄ±p maliyet hesaplayacaÄŸÄ±z
        for i in range(0, n, batch_size):
            # rasgele X_i ve y_i partileri
            X_i = X[i:i+batch_size]
            y_i = y[i:i+batch_size]
        
            # bias column ekle (1'ler sÃ¼tunu)
            x_i = np.c_[np.ones(len(X_i)), X_i]
        
            # X_i partisi iÃ§in tahmin
            tahmin = np.dot(X_i, W)

            # X_i partisi iÃ§in katsayÄ± deÄŸiÅŸimi
            W = W - (1/n) * learning_rate * (X_i.T.dot((tahmin - y_i)))

            # X_i partisi iÃ§in hesaplanan cost'u cost deÄŸiÅŸkenine ekle
            cost += cal_cost(W, X_i, y_i)
        
        # bu iterasyon iÃ§in cost deÄŸerini cost_history'ye ekle
        cost_history[it] = cost
    
    return W, cost_history
    
    
    
 
 # learning rate
lr = 0.1

# iterasyon sayÄ±sÄ±
n_iter = 200

# baÅŸlangÄ±Ã§ iÃ§in W vektÃ¶rÃ¼
W = np.random.randn(2,1)

# X vektÃ¶rÃ¼ne bias column ekle
X_b = np.c_[np.ones((len(X),1)),X]

# gradient descent Ã§alÄ±ÅŸtÄ±r
W, cost_history = minibatch_stochastic_gradient_descent(X_b, y, W, lr, n_iter)


print('W0: {:0.3f}'.format(W[0][0]))
print('W1: {:0.3f}'.format(W[1][0]))
print('Final cost/MSE: {:0.3f}'.format(cost_history[-1]))




fig,ax = plt.subplots(figsize=(10,8))

ax.set_ylabel('{J(W)}',rotation=0)
ax.set_xlabel('{Iterations}')

_=ax.plot(range(n_iter), cost_history, 'b.')




